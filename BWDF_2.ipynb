{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlySalem94/UIC-SWMM_BWDF/blob/main/BWDF_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Multivariate LSTM Model for Short-term Water Demand Forecasting**\n",
        "\n",
        "***Battle of Water Demand Forecast - CCWI 2024***\n",
        "\n",
        "**Team Name: UIC-SWIM**\n",
        "\n",
        "**Team Members:**\n",
        "1.   [Aly Salem](https://www.linkedin.com/in/aly-salem-70b97813b/) - asalem22@uic.edu\n",
        "2.   [Ahmed Abokifa](https://www.linkedin.com/in/ahmed-abokifa/) - abokifa@uic.edu"
      ],
      "metadata": {
        "id": "rzfE_uqzHwui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Necessary Libraries"
      ],
      "metadata": {
        "id": "UGYwNKsbJhwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import plotly.io as pio\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-ckFjVlnJmI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Inputs and Training Parameters"
      ],
      "metadata": {
        "id": "_LnWNDYqUlRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define run details\n",
        "run_name = 'A - weather - PI'\n",
        "demand_to_keep = ['DMA A (L/s)']\n",
        "weather_to_keep = ['Rainfall depth (mm)', 'Air temperature (Â°C)', 'Air humidity (%)', 'Windspeed (km/h)']\n",
        "\n",
        "# Define file paths and date format\n",
        "demand_path = 'Inflow.xlsx'\n",
        "weather_path = 'Weather.xlsx'\n",
        "holiday_path = 'Holidays.xlsx'\n",
        "date_format = '%d/%m/%Y %H:%M'\n",
        "start_date = '2021-01-01 00:00:00'\n",
        "end_date = '2022-07-24 23:00:00'\n",
        "\n",
        "# Define training parameters\n",
        "input_sequences = 168*4\n",
        "output_sequence = 168\n",
        "test_days = 168\n",
        "n_epochs = 1000\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "hidden_size = 50\n",
        "num_classes = output_sequence\n",
        "num_layers = 2\n",
        "\n",
        "colab = True\n",
        "filling_days = 7"
      ],
      "metadata": {
        "id": "_X7SeFCKKB4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Input Data"
      ],
      "metadata": {
        "id": "oKqPom8qVebN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get/Create a directory for data/results\n",
        "dt_string = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
        "\n",
        "if not colab:\n",
        "  data_dir = os.path.join(os.path.dirname(__file__), 'data')\n",
        "  os.mkdir(os.path.join(os.path.dirname(__file__), f\"results\\\\{dt_string}_{run_name}\"))\n",
        "  results_dir = os.path.join(os.path.dirname(__file__), f\"results\\\\{dt_string}_{run_name}\")\n",
        "\n",
        "else:\n",
        "  data_dir = os.path.join('/content/gdrive/MyDrive/Colab Notebooks', 'Battle')\n",
        "  os.mkdir(os.path.join('/content/gdrive/MyDrive/Colab Notebooks/results', f\"{dt_string}_{run_name}\"))\n",
        "  results_dir = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/results', f\"{dt_string}_{run_name}\")\n",
        "\n",
        "# Load data from Excel files\n",
        "demand_df = pd.read_excel(demand_path, parse_dates=[0], index_col=0)\n",
        "weather_df = pd.read_excel(weather_path, parse_dates=[0], index_col=0)\n",
        "holiday_df = pd.read_excel(holiday_path, parse_dates=[0], index_col=0)\n",
        "\n",
        "# Convert index to datetime format\n",
        "demand_df.index = pd.to_datetime(demand_df.index, format=date_format)\n",
        "weather_df.index = pd.to_datetime(weather_df.index, format=date_format)\n",
        "holiday_df.index = pd.to_datetime(holiday_df.index, format=date_format)\n",
        "\n",
        "# Merge dataframes on the date index\n",
        "data_df = pd.merge(demand_df, weather_df, left_index=True, right_index=True, how='outer')\n",
        "data_df = pd.merge(data_df, holiday_df, left_index=True, right_index=True, how='outer')\n",
        "\n",
        "# Select data within the specified date range\n",
        "data_df = data_df.loc[start_date:end_date]"
      ],
      "metadata": {
        "id": "ClmLrJK2Ks6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manage Summer and Standard time"
      ],
      "metadata": {
        "id": "1fIWeSkVV0CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle standard time (2 am is repeated at 10/31/2021 & 10/30/2022)\n",
        "data_df = data_df[~data_df.index.duplicated(keep='first')]\n",
        "\n",
        "# Handle summer time (2 am is missed hour at 3/28/2021 & 3/27/2022)\n",
        "summer_index = pd.to_datetime(['2021-03-28 02:00:00', '2022-03-27 02:00:00'])\n",
        "summer_df = pd.DataFrame(index=summer_index)\n",
        "data_df = pd.concat([data_df, summer_df])\n",
        "\n",
        "# Sort the index to maintain order\n",
        "data_df.sort_index(inplace=True)\n",
        "data_df_original = data_df.copy()"
      ],
      "metadata": {
        "id": "TIZE28tCTrJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Run Details to a Results File"
      ],
      "metadata": {
        "id": "0ggC3sCcXHIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "    f.write(f\"run_name = '{dt_string}_{run_name}'\\n\")\n",
        "    f.write(f\"demand_to_keep = {demand_to_keep}\\n\")\n",
        "    f.write(f\"weather_to_keep = {weather_to_keep}\\n\\n\")\n",
        "    f.write(f\"input_sequences = {input_sequences}\\n\")\n",
        "    f.write(f\"output_sequence = {output_sequence}\\n\")\n",
        "    f.write(f\"test_days = {test_days}\\n\")\n",
        "    f.write(f\"n_epochs = {n_epochs}\\n\")\n",
        "    f.write(f\"batch_size = {batch_size}\\n\")\n",
        "    f.write(f\"learning_rate = {learning_rate}\\n\\n\")\n",
        "    f.write(f\"hidden_size = {hidden_size}\\n\")\n",
        "    f.write(f\"num_classes = {num_classes}\\n\")\n",
        "    f.write(f\"num_layers = {num_layers}\\n\\n\")\n",
        "    f.write(f\"demand_path = '{demand_path}'\\n\")\n",
        "    f.write(f\"weather_path = '{weather_path}'\\n\")\n",
        "    f.write(f\"date_format = '{date_format}'\\n\\n\")\n",
        "    f.write(f\"start_date = '{start_date}'\\n\")\n",
        "    f.write(f\"end_date = '{end_date}'\\n\\n\")\n",
        "    f.write(\"_____________________________________________________________\\n\")\n",
        ""
      ],
      "metadata": {
        "id": "ZNLcluymTzMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Values Using Wieghted Moving Mean"
      ],
      "metadata": {
        "id": "4beY1mgyXT-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each input column\n",
        "for column in data_df.columns.values:\n",
        "    # Create a date index from start to end dates\n",
        "    date_index = pd.date_range(start=data_df.index[0].date(), end=data_df.index[-1].date(), freq='D')\n",
        "\n",
        "    # Create an hourly DataFrame with 24 columns representing hours in each day\n",
        "    hourly_df = pd.DataFrame(index=date_index, columns=range(0, 24))\n",
        "    hourly_df = hourly_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Fill the hourly DataFrame with values from the original data_df\n",
        "    for date in hourly_df.index:\n",
        "        for hour in range(0, 24):\n",
        "            hourly_df.loc[date, hour] = data_df.loc[date + pd.Timedelta(hours=hour), column]\n",
        "\n",
        "    for hour in hourly_df.columns:\n",
        "        missing_dates = hourly_df[hour].isnull()\n",
        "\n",
        "        for missing_index in hourly_df.index[missing_dates]:\n",
        "            dates_upto_missing_date = pd.date_range(start=pd.to_datetime(start_date).replace(hour=missing_index.hour), end=missing_index, freq=pd.Timedelta(hours=24))\n",
        "\n",
        "            previous_values = hourly_df.loc[dates_upto_missing_date[-(filling_days)-1:-1], hour]\n",
        "\n",
        "            weights = (np.array([*range(1, len(previous_values)+1)])/np.array([*range(1, len(previous_values)+1)]).sum())\n",
        "\n",
        "            hourly_df.loc[missing_index, hour] = (previous_values*weights).sum()\n",
        "\n",
        "    # Reconstrcut the data_df with the filled values\n",
        "    data_df[column] = hourly_df.values.reshape(-1)"
      ],
      "metadata": {
        "id": "Wy_PNVbKJyFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract/Add Hour, Day, and Weekday Features"
      ],
      "metadata": {
        "id": "k05t90_cOd_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['hour'] = data_df.index.hour\n",
        "data_df['day_of_year'] = data_df.index.dayofyear\n",
        "data_df['weekday'] = data_df.index.weekday"
      ],
      "metadata": {
        "id": "geGJOrCmXne1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardize Input/Output, Apply PCA and Data Transformation"
      ],
      "metadata": {
        "id": "O_PzIAM0Ov5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_input = data_df\n",
        "y_input = np.array(data_df[demand_to_keep])\n",
        "X_input.shape, y_input.shape\n",
        "\n",
        "mm = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "X_std = ss.fit_transform(X_input)\n",
        "y_std = mm.fit_transform(y_input.reshape(-1, 1))\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_std)\n",
        "\n",
        "X_std = pca.transform(X_std)"
      ],
      "metadata": {
        "id": "OtiWdfOmOwLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into Input/Output Sequences"
      ],
      "metadata": {
        "id": "gKS4tNiGP2KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split a multivariate sequence into samples\n",
        "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list() # instantiate X and y\n",
        "    for i in range(len(input_sequences)):\n",
        "        # find the end of the input, output sequence\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "        # check if we are beyond the dataset\n",
        "        if out_end_ix > len(input_sequences): break\n",
        "        # gather input and output of the pattern\n",
        "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix:out_end_ix, -1]\n",
        "        X.append(seq_x), y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = split_sequences(X_std, y_std, input_sequences, output_sequence)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "llgZjSrtP2T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Data to Tensors"
      ],
      "metadata": {
        "id": "-lsdtABXQG_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "total_samples = len(X)\n",
        "train_test_cutoff = X.shape[0]-1\n",
        "\n",
        "X_train, X_test = torch.Tensor(X[0:train_test_cutoff]).to(device), torch.Tensor(X[train_test_cutoff:]).to(device)\n",
        "y_train, y_test = torch.Tensor(y[0:train_test_cutoff]).to(device), torch.Tensor(y[train_test_cutoff:]).to(device)\n",
        "\n",
        "print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Shape:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "rucfxQogMtbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define The LSTM Model and The Loss Function"
      ],
      "metadata": {
        "id": "rVs-cSbwQyD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The LSTM model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes # output size\n",
        "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
        "        self.input_size = input_size # input size\n",
        "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
        "        # LSTM model\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
        "        self.fc_1 =  nn.Linear(hidden_size, 128) # fully connected\n",
        "        self.fc_2 = nn.Linear(128, num_classes) # fully connected last layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.to(device)\n",
        "        # hidden state\n",
        "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # cell state\n",
        "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
        "        hn = hn[-1,:,:].view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        out = self.fc_1(out) # first dense\n",
        "        out = self.relu(out) # relu\n",
        "        out = self.fc_2(out) # final output\n",
        "        return out\n",
        "\n",
        "# The Loss Function\n",
        "class PI(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PI, self).__init__()\n",
        "\n",
        "    def forward(self, predicted_values, true_values):\n",
        "\n",
        "        true_values = true_values.reshape(-1,7,24)\n",
        "\n",
        "        predicted_values = predicted_values.reshape(-1,7,24)\n",
        "\n",
        "        absolute_diff = torch.abs(true_values - predicted_values)\n",
        "\n",
        "        PI1 = absolute_diff.mean(dim=2).mean(dim=1)\n",
        "        PI2 = absolute_diff[:, 0, :].max(dim=1).values\n",
        "        PI3 = absolute_diff[:, 1:, :].mean(dim=2).mean(dim=1)\n",
        "\n",
        "        return PI1.mean() + PI2.mean() + PI3.mean()"
      ],
      "metadata": {
        "id": "sGuLur6sQUit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define The Training Loop and The Performance Indicators (PI)"
      ],
      "metadata": {
        "id": "51V6iuypROst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train,\n",
        "                  X_test, y_test, batch_size):\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        lstm.train()\n",
        "\n",
        "        total_train_loss = 0.0  # Accumulate training loss over all mini-batches\n",
        "\n",
        "        # Iterate over mini-batches\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            X_batch = X_train[i:i+batch_size]\n",
        "            y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "            outputs = lstm.forward(X_batch)  # forward pass\n",
        "            optimiser.zero_grad()  # calculate the gradient, manually setting to 0\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            loss.backward()  # calculates the loss of the loss function\n",
        "            optimiser.step()  # improve from loss, i.e backprop\n",
        "\n",
        "            total_train_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "        average_train_loss = total_train_loss / (len(X_train) / batch_size)\n",
        "\n",
        "        # test loss\n",
        "        lstm.eval()\n",
        "        test_preds = lstm(X_test)\n",
        "        test_loss = loss_fn(test_preds, y_test)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            # Print average time over the last 100 epochs\n",
        "            print(\"Epoch: %d, train loss/mini patch: %1.5f, test loss for all test days: %1.5f\" % (epoch, average_train_loss, test_loss.item()))\n",
        "\n",
        "            with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "                f.write(\"Epoch: %d, train loss/mini patch: %1.5f, test loss for all test days: %1.5f \\n\" % (epoch, average_train_loss, test_loss.item()))\n",
        "\n",
        "# The Performance Indicators (PI)\n",
        "def PI_test(predicted_values, true_values):\n",
        "\n",
        "    true_values = torch.from_numpy(true_values)\n",
        "    predicted_values = torch.from_numpy(predicted_values)\n",
        "\n",
        "    true_values = true_values.reshape(-1,7,24)\n",
        "\n",
        "    predicted_values = predicted_values.reshape(-1,7,24)\n",
        "\n",
        "    absolute_diff = torch.abs(true_values - predicted_values)\n",
        "\n",
        "    PI1 = absolute_diff.mean(dim=2).mean(dim=1)\n",
        "    PI2 = absolute_diff[:, 0, :].max(dim=1).values\n",
        "    PI3 = absolute_diff[:, 1:, :].mean(dim=2).mean(dim=1)\n",
        "\n",
        "    return PI1.mean(), PI2.mean(), PI3.mean()\n"
      ],
      "metadata": {
        "id": "7J18BjTwQ_Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training/Testing"
      ],
      "metadata": {
        "id": "-oo-_DDnSDOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the LSTM model\n",
        "lstm = LSTM(num_classes, np.size(X,2), hidden_size, num_layers)\n",
        "loss_fn = PI()\n",
        "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train the LSTM model\n",
        "t1 = time.time()\n",
        "print(f'run: {run_name} started at: {dt_string}')\n",
        "training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test, batch_size)\n",
        "t2 = time.time()\n",
        "print(f'run: {dt_string}_{run_name}, ended in: {(t2-t1):.2f} sec.')\n",
        "with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "    f.writelines('_____________________________________________________________'+ '\\n')\n",
        "    f.write(f\"run: {dt_string}, ended in: {(t2-t1):.2f} sec.\\n\")\n",
        "\n",
        "# Save the trained model\n",
        "path = os.path.join(results_dir, 'LSTM_model')\n",
        "torch.save(lstm.state_dict(), path)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted_test = lstm(X_test).to('cpu').detach().numpy()\n",
        "predicted_test = mm.inverse_transform(predicted_test)\n",
        "actual_test = mm.inverse_transform(y_test.to('cpu'))\n",
        "\n",
        "# Calculate the PI\n",
        "PI1, PI2, PI3 = PI_test(actual_test, predicted_test)\n",
        "\n",
        "print(f'PI1 : {PI1}, PI2 : {PI2}, PI3 : {PI3}')\n",
        "with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "    f.writelines('_____________________________________________________________'+ '\\n')\n",
        "    f.write(f\"PI1 : {PI1}, PI2 : {PI2}, PI3 : {PI3}\\n\")"
      ],
      "metadata": {
        "id": "2NyKgrKERA2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Model Results"
      ],
      "metadata": {
        "id": "4qT4pRCQSpUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs. predicted values\n",
        "start_date = datetime(2021, 1, 1) + timedelta(hours=input_sequences)\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(len(y-output_sequence))]\n",
        "start_date = date_values[-1]\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(output_sequence)]\n",
        "\n",
        "trace_actual = go.Scatter(x=date_values, y=actual_test.reshape(-1), mode='lines', name='Actual Data')\n",
        "trace_predicted = go.Scatter(x=date_values, y=predicted_test.reshape(-1), mode='lines', name='Predicted Data')\n",
        "\n",
        "layout = go.Layout(title=run_name, legend=dict(x=0, y=1, traceorder='normal', orientation='h'))\n",
        "fig = go.Figure(data=[trace_actual, trace_predicted], layout=layout)\n",
        "\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "U-I8xn9jL9X3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}