{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlySalem94/UIC-SWMM_BWDF/blob/main/BWDF_Online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzfE_uqzHwui",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **A Multivariate LSTM Model for Short-term Water Demand Forecasting**\n",
        "\n",
        "***Battle of Water Demand Forecast - CCWI 2024***\n",
        "\n",
        "**Team Name: UIC-SWIM**\n",
        "\n",
        "**Team Members:**\n",
        "1.   [Aly Salem](https://www.linkedin.com/in/aly-salem-70b97813b/) - Ph.D. Candidate, Department of Civil, Materials, and Environmental Engineering, The University of Illinois Chicago - asalem22@uic.edu\n",
        "2.   [Ahmed Abokifa](https://www.linkedin.com/in/ahmed-abokifa/) - Assistant Professor, Department of Civil, Materials, and Environmental Engineering, The University of Illinois Chicago - abokifa@uic.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGYwNKsbJhwb"
      },
      "source": [
        "# Update Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-ckFjVlnJmI4"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZG8iwPrqpIb"
      },
      "source": [
        "# Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BpNiZEAoqpIb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LnWNDYqUlRi"
      },
      "source": [
        "# Define Inputs and Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_X7SeFCKKB4q"
      },
      "outputs": [],
      "source": [
        "# Define DMA name\n",
        "DMA_name = 'A'\n",
        "\n",
        "# Define file paths and date format\n",
        "demand_file = 'Inflow.xlsx'\n",
        "weather_file = 'Weather.xlsx'\n",
        "holidays_file = 'Holidays.xlsx'\n",
        "\n",
        "# Define training parameters\n",
        "input_sequence = 168 # in hours\n",
        "output_sequence = 168 # in hours\n",
        "n_epochs = 1000\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "hidden_size = [10,128]\n",
        "num_layers = 2\n",
        "\n",
        "colab = False\n",
        "filling_days = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKqPom8qVebN"
      },
      "source": [
        "# Load and Prepare Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ClmLrJK2Ks6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7419a5-e078-43b2-e95c-e5d2c61db353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UIC-SWMM_BWDF'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (16/16), 1.71 MiB | 11.23 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AlySalem94/UIC-SWMM_BWDF.git\n",
        "\n",
        "data_dir = os.path.join('/content/UIC-SWMM_BWDF')\n",
        "\n",
        "# Load data from Excel files\n",
        "demand_df = pd.read_excel(f\"{data_dir}/{demand_file}\", parse_dates=[0], index_col=0)\n",
        "weather_df = pd.read_excel(f\"{data_dir}/{weather_file}\", parse_dates=[0], index_col=0)\n",
        "holiday_df = pd.read_excel(f\"{data_dir}/{holidays_file}\", parse_dates=[0], index_col=0)\n",
        "\n",
        "# Assign DMA names and meteorological data to demand_df and weather_df\n",
        "demand_df.columns = [chr(ord('A') + i) for i in range(len(demand_df.columns))]\n",
        "weather_df.columns = ['Rainfall depth', 'Air temperature', 'Air humidity', 'Windspeed']\n",
        "\n",
        "# Convert index to datetime format\n",
        "date_format = '%d/%m/%Y %H:%M'\n",
        "start_date = '2021-01-01 00:00:00'\n",
        "end_date = '2022-07-24 23:00:00'\n",
        "demand_df.index = pd.to_datetime(demand_df.index, format=date_format)\n",
        "weather_df.index = pd.to_datetime(weather_df.index, format=date_format)\n",
        "holiday_df.index = pd.to_datetime(holiday_df.index, format=date_format)\n",
        "\n",
        "# Merge dataframes on the date index\n",
        "data_df = pd.merge(demand_df[DMA_name], weather_df, left_index=True, right_index=True, how='outer')\n",
        "data_df = pd.merge(data_df, holiday_df, left_index=True, right_index=True, how='outer')\n",
        "\n",
        "# Select data within the specified date range\n",
        "data_df = data_df.loc[start_date:end_date]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80tcCAkSqpIc"
      },
      "source": [
        "# Create a Directory for Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5Z5pBimvqpId"
      },
      "outputs": [],
      "source": [
        "# Get the the run time\n",
        "dt_string = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Check if the 'Results' folder exists, and create it if not\n",
        "results_folder = os.path.join('/content/UIC-SWMM_BWDF', 'Results')\n",
        "if not os.path.exists(results_folder):\n",
        "    os.makedirs(results_folder)\n",
        "\n",
        "# Create a folder with the DMA name and run time\n",
        "os.makedirs(os.path.join('/content/UIC-SWMM_BWDF', 'Results', f\"{DMA_name}_{dt_string}\"))\n",
        "\n",
        "results_dir = os.path.join('/content/UIC-SWMM_BWDF', 'Results', f\"{DMA_name}_{dt_string}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fIWeSkVV0CS"
      },
      "source": [
        "# Manage Summer and Standard time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TIZE28tCTrJE"
      },
      "outputs": [],
      "source": [
        "# Handle standard time (2 am is repeated at 10/31/2021 & 10/30/2022)\n",
        "data_df = data_df[~data_df.index.duplicated(keep='first')]\n",
        "\n",
        "# Handle summer time (2 am is missed hour at 3/28/2021 & 3/27/2022)\n",
        "summer_index = pd.to_datetime(['2021-03-28 02:00:00', '2022-03-27 02:00:00'])\n",
        "summer_df = pd.DataFrame(index=summer_index)\n",
        "data_df = pd.concat([data_df, summer_df])\n",
        "\n",
        "# Sort the index to maintain order\n",
        "data_df.sort_index(inplace=True)\n",
        "data_df_original = data_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ggC3sCcXHIQ"
      },
      "source": [
        "# Save Run Details to a Results File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZNLcluymTzMV"
      },
      "outputs": [],
      "source": [
        "with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "    f.write(f\"DMA_name = {DMA_name}\\n\")\n",
        "    f.write(f\"input_sequence = {input_sequence}\\n\")\n",
        "    f.write(f\"output_sequence = {output_sequence}\\n\")\n",
        "    f.write(f\"n_epochs = {n_epochs}\\n\")\n",
        "    f.write(f\"batch_size = {batch_size}\\n\")\n",
        "    f.write(f\"learning_rate = {learning_rate}\\n\\n\")\n",
        "    f.write(f\"hidden_size = {hidden_size}\\n\")\n",
        "    f.write(f\"num_layers = {num_layers}\\n\\n\")\n",
        "    f.write(f\"demand_file = '{demand_file}'\\n\")\n",
        "    f.write(f\"weather_file = '{weather_file}'\\n\")\n",
        "    f.write(f\"date_format = '{date_format}'\\n\\n\")\n",
        "    f.write(f\"start_date = '{start_date}'\\n\")\n",
        "    f.write(f\"end_date = '{end_date}'\\n\\n\")\n",
        "    f.write(\"_____________________________________________________________\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4beY1mgyXT-V"
      },
      "source": [
        "# Handle Missing Values Using Wieghted Moving Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wy_PNVbKJyFs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "dccf9edf-8c5b-4515-9f44-4b3699cdaecb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Timestamp('2022-01-08 00:00:00', freq='D')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1641600000000000000",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2022-01-08 00:00:00', freq='D')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2022-01-08 00:00:00', freq='D')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0059dd230c3e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhourly_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhour\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mhourly_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhour\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhourly_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;31m#  results if our categories are integers that dont match our codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3923\u001b[0m             \u001b[0;31m# IntervalIndex: IntervalTree has no get_loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3924\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_cast_for_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2022-01-08 00:00:00', freq='D')"
          ]
        }
      ],
      "source": [
        "# Loop through each input column\n",
        "for column in data_df.columns.values:\n",
        "    # Create a date index from start to end dates\n",
        "    date_index = pd.date_range(start=data_df.index[0].date(), end=data_df.index[-1].date(), freq='D')\n",
        "\n",
        "    # Create an hourly DataFrame with 24 columns representing hours in each day\n",
        "    hourly_df = pd.DataFrame(index=date_index, columns=range(0, 24))\n",
        "    hourly_df = hourly_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Fill the hourly DataFrame with values from the original data_df\n",
        "    for date in hourly_df.index:\n",
        "        for hour in range(0, 24):\n",
        "            hourly_df.loc[date, hour] = data_df.loc[date + pd.Timedelta(hours=hour), column]\n",
        "\n",
        "    for hour in hourly_df.columns:\n",
        "        missing_dates = hourly_df[hour].isnull()\n",
        "\n",
        "        for missing_index in hourly_df.index[missing_dates]:\n",
        "            dates_upto_missing_date = pd.date_range(start=pd.to_datetime(start_date).replace(hour=missing_index.hour), end=missing_index, freq=pd.Timedelta(hours=24))\n",
        "\n",
        "            previous_values = hourly_df.loc[dates_upto_missing_date[-(filling_days)-1:-1], hour]\n",
        "\n",
        "            weights = (np.array([*range(1, len(previous_values)+1)])/np.array([*range(1, len(previous_values)+1)]).sum())\n",
        "\n",
        "            hourly_df.loc[missing_index, hour] = (previous_values*weights).sum()\n",
        "\n",
        "    # Reconstrcut the data_df with the filled values\n",
        "    data_df[column] = hourly_df.values.reshape(-1)\n",
        "\n",
        "start_date = datetime(2021, 1, 1)\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(len(data_df_original))]\n",
        "\n",
        "\n",
        "trace_original = go.Scatter(x=date_values, y=data_df_original.loc[start_date:date_values[-1]].values[:,0], mode='lines', name='Original')\n",
        "\n",
        "trace_filled = go.Scatter(x=date_values, y=data_df.loc[start_date:date_values[-1]].values[:,0], mode='lines', name='Original')\n",
        "\n",
        "layout = go.Layout(title='', legend=dict(x=0, y=1, traceorder='normal', orientation='h'))\n",
        "\n",
        "fig = go.Figure(data=[trace_filled, trace_original], layout=layout)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k05t90_cOd_V"
      },
      "source": [
        "# Extract/Add Hour, Day, and Weekday Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geGJOrCmXne1"
      },
      "outputs": [],
      "source": [
        "data_df['hour'] = data_df.index.hour\n",
        "data_df['day_of_year'] = data_df.index.dayofyear\n",
        "data_df['weekday'] = data_df.index.weekday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PzIAM0Ov5R"
      },
      "source": [
        "# Standardize Input/Output, Apply PCA and Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtiWdfOmOwLu"
      },
      "outputs": [],
      "source": [
        "X_input = data_df\n",
        "y_input = np.array(data_df[DMA_name])\n",
        "X_input.shape, y_input.shape\n",
        "\n",
        "mm = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "\n",
        "X_std = ss.fit_transform(X_input)\n",
        "y_std = mm.fit_transform(y_input.reshape(-1, 1))\n",
        "# y_std = y_input.reshape(-1, 1)\n",
        "\n",
        "pca = PCA(4)\n",
        "pca.fit(X_std)\n",
        "\n",
        "X_std = pca.transform(X_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKS4tNiGP2KQ"
      },
      "source": [
        "# Split Data into Input/Output Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llgZjSrtP2T4"
      },
      "outputs": [],
      "source": [
        "# split a multivariate sequence into samples\n",
        "def split_sequences(input_sequence, output_sequence, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list() # instantiate X and y\n",
        "    for i in range(len(input_sequence)):\n",
        "        # find the end of the input, output sequence\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "        # check if we are beyond the dataset\n",
        "        if out_end_ix > len(input_sequence): break\n",
        "        # gather input and output of the pattern\n",
        "        seq_x, seq_y = input_sequence[i:end_ix], output_sequence[end_ix:out_end_ix, -1]\n",
        "        X.append(seq_x), y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = split_sequences(X_std, y_std, input_sequence, output_sequence)\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lsdtABXQG_U"
      },
      "source": [
        "# Convert Data to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rucfxQogMtbM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "total_samples = len(X)\n",
        "train_test_cutoff = X.shape[0]-input_sequence\n",
        "\n",
        "X_train, X_test = torch.Tensor(X[0:train_test_cutoff]).to(device), torch.Tensor(X[train_test_cutoff:])[-1].reshape(1,input_sequence,-1).to(device)\n",
        "y_train, y_test = torch.Tensor(y[0:train_test_cutoff]).to(device), torch.Tensor(y[train_test_cutoff:])[-1].reshape(1,-1).to(device)\n",
        "\n",
        "print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Shape:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVs-cSbwQyD9"
      },
      "source": [
        "# Define The LSTM Model and The Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEgT22BIqpIf"
      },
      "outputs": [],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGuLur6sQUit"
      },
      "outputs": [],
      "source": [
        "# The LSTM model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes # output size\n",
        "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
        "        self.input_size = input_size # input size\n",
        "        self.hidden_size = hidden_size[0] # neurons in each lstm layer\n",
        "        # LSTM model\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size[0],\n",
        "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
        "        self.fc_1 =  nn.Linear(hidden_size[0], hidden_size[1]) # fully connected\n",
        "        self.fc_2 = nn.Linear(hidden_size[1], num_classes) # fully connected last layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.to(device)\n",
        "        # hidden state\n",
        "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # cell state\n",
        "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
        "        hn = hn[-1,:,:].view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        out = self.fc_1(out) # first dense\n",
        "        out = self.relu(out) # relu\n",
        "        out = self.fc_2(out) # final output\n",
        "        return out\n",
        "\n",
        "# The Loss Function\n",
        "class PI(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PI, self).__init__()\n",
        "\n",
        "    def forward(self, predicted_values, true_values):\n",
        "\n",
        "        true_values = true_values.reshape(-1,7,24)\n",
        "\n",
        "        predicted_values = predicted_values.reshape(-1,7,24)\n",
        "\n",
        "        absolute_diff = torch.abs(true_values - predicted_values)\n",
        "\n",
        "        PI1 = absolute_diff.mean(dim=2).mean(dim=1)\n",
        "        PI2 = absolute_diff[:, 0, :].max(dim=1).values\n",
        "        PI3 = absolute_diff[:, 1:, :].mean(dim=2).mean(dim=1)\n",
        "\n",
        "        return PI1.mean() + PI2.mean() + PI3.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51V6iuypROst"
      },
      "source": [
        "# Define The Training Loop and The Performance Indicators (PI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J18BjTwQ_Gb"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train,\n",
        "                  X_test, y_test, batch_size):\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        lstm.train()\n",
        "\n",
        "        total_train_loss = 0.0  # Accumulate training loss over all mini-batches\n",
        "\n",
        "        # Iterate over mini-batches\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            X_batch = X_train[i:i+batch_size]\n",
        "            y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "            outputs = lstm.forward(X_batch)  # forward pass\n",
        "            optimiser.zero_grad()  # calculate the gradient, manually setting to 0\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            loss.backward()  # calculates the loss of the loss function\n",
        "            optimiser.step()  # improve from loss, i.e backprop\n",
        "\n",
        "            total_train_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "        average_train_loss = total_train_loss / (len(X_train) / batch_size)\n",
        "\n",
        "        # test loss\n",
        "        lstm.eval()\n",
        "        test_preds = lstm(X_test)\n",
        "        test_loss = loss_fn(test_preds, y_test)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            # Print average time over the last 100 epochs\n",
        "            print(\"Epoch: %d, train loss/mini patch: %1.5f, test loss for W29/2022: %1.5f\" % (epoch, average_train_loss, test_loss.item()))\n",
        "\n",
        "            with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "                f.write(\"Epoch: %d, train loss/mini patch: %1.5f, test loss for W29/2022: %1.5f \\n\" % (epoch, average_train_loss, test_loss.item()))\n",
        "\n",
        "# The Performance Indicators (PI)\n",
        "def PI_test(predicted_values, true_values):\n",
        "\n",
        "    true_values = torch.from_numpy(true_values)\n",
        "    predicted_values = torch.from_numpy(predicted_values)\n",
        "\n",
        "    true_values = true_values.reshape(-1,7,24)\n",
        "\n",
        "    predicted_values = predicted_values.reshape(-1,7,24)\n",
        "\n",
        "    absolute_diff = torch.abs(true_values - predicted_values)\n",
        "\n",
        "    PI1 = absolute_diff.mean(dim=2).mean(dim=1)\n",
        "    PI2 = absolute_diff[:, 0, :].max(dim=1).values\n",
        "    PI3 = absolute_diff[:, 1:, :].mean(dim=2).mean(dim=1)\n",
        "\n",
        "    return PI1.mean(), PI2.mean(), PI3.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oo-_DDnSDOh"
      },
      "source": [
        "# Model Training/Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NyKgrKERA2C"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the LSTM model\n",
        "lstm = LSTM(output_sequence, np.size(X,2), hidden_size, num_layers)\n",
        "loss_fn = PI()\n",
        "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train the LSTM model\n",
        "t1 = time.time()\n",
        "print(f'run: {DMA_name}_{dt_string} started at: {dt_string}')\n",
        "training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test, batch_size)\n",
        "t2 = time.time()\n",
        "print(f'run: {DMA_name}_{dt_string} ended in: {(t2-t1):.2f} sec.')\n",
        "with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "    f.writelines('_____________________________________________________________'+ '\\n')\n",
        "    f.write(f\"run: {dt_string}, ended in: {(t2-t1):.2f} sec.\\n\")\n",
        "\n",
        "# Save the trained model\n",
        "path = os.path.join(results_dir, 'LSTM_model')\n",
        "torch.save(lstm.state_dict(), path)\n",
        "\n",
        "# # Load the model if already trained\n",
        "# path = os.path.join(os.getcwd(), 'results', 'A_01-31_14-14-02', 'LSTM_model')\n",
        "# model_data = torch.load(path, map_location=device)\n",
        "# lstm.load_state_dict(model_data)\n",
        "\n",
        "# Make predictions for W29/2022\n",
        "lstm.eval()\n",
        "predicted_test = lstm(X_test).to('cpu').detach().numpy()\n",
        "predicted_test = mm.inverse_transform(predicted_test)\n",
        "actual_test = mm.inverse_transform(y_test.to('cpu'))\n",
        "\n",
        "# Calculate the PI\n",
        "PI1, PI2, PI3 = PI_test(actual_test, predicted_test)\n",
        "\n",
        "print(f'PI1 : {PI1}, PI2 : {PI2}, PI3 : {PI3}')\n",
        "with open(f\"{results_dir}/Results.txt\", \"a\") as f:\n",
        "    f.writelines('_____________________________________________________________'+ '\\n')\n",
        "    f.write(f\"PI1 : {PI1}, PI2 : {PI2}, PI3 : {PI3}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qT4pRCQSpUJ"
      },
      "source": [
        "# Plot Model Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-I8xn9jL9X3"
      },
      "outputs": [],
      "source": [
        "# Plot actual vs. predicted values for W29/2022\n",
        "start_date = datetime(2021, 1, 1) + timedelta(hours=input_sequence)\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(len(y))]\n",
        "\n",
        "start_date = date_values[-1]\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(output_sequence)]\n",
        "\n",
        "trace_actual = go.Scatter(x=date_values, y=actual_test.reshape(-1), mode='lines', name='Actual Data')\n",
        "trace_predicted = go.Scatter(x=date_values, y=predicted_test.reshape(-1), mode='lines', name='Predicted Data')\n",
        "\n",
        "layout = go.Layout(title='WDF for W29/2022', legend=dict(x=0, y=1, traceorder='normal', orientation='h'))\n",
        "fig = go.Figure(data=[trace_actual, trace_predicted], layout=layout)\n",
        "\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9z0B9m_qpIg"
      },
      "outputs": [],
      "source": [
        "X_forecast = torch.Tensor(pca.transform(ss.transform(X_input[-input_sequence-output_sequence:-output_sequence].values)).reshape(1,input_sequence,-1)).to(device)\n",
        "\n",
        "# Make predictions for W30/2022\n",
        "lstm.eval()\n",
        "predicted_forecast = lstm(X_forecast).to('cpu').detach().numpy()\n",
        "predicted_forecast = mm.inverse_transform(predicted_forecast)\n",
        "\n",
        "\n",
        "# Plot forecast values for W30/2022\n",
        "start_date = datetime(2021, 1, 1) + timedelta(hours=input_sequence)\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(len(y)+output_sequence)]\n",
        "start_date = date_values[-1]\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(output_sequence)]\n",
        "\n",
        "trace_predicted = go.Scatter(x=date_values, y=predicted_forecast.reshape(-1), mode='lines', name='Predicted Data')\n",
        "\n",
        "layout = go.Layout(title='WDF for W30/2022', legend=dict(x=0, y=1, traceorder='normal', orientation='h'))\n",
        "fig = go.Figure(data=[trace_predicted], layout=layout)\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rCYoUiOqpIg"
      },
      "outputs": [],
      "source": [
        "X_forecast = torch.Tensor(pca.transform(ss.transform(X_input[-input_sequence:].values)).reshape(1,(input_sequence),-1)).to(device)\n",
        "\n",
        "# Make predictions for W30/2022\n",
        "lstm.eval()\n",
        "predicted_forecast = lstm(X_forecast).to('cpu').detach().numpy()\n",
        "predicted_forecast = mm.inverse_transform(predicted_forecast)\n",
        "\n",
        "\n",
        "# Plot forecast values for W30/2022\n",
        "start_date = datetime(2021, 1, 1) + timedelta(hours=input_sequence)\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(len(y)+output_sequence)]\n",
        "start_date = date_values[-1]\n",
        "date_values = [start_date + timedelta(hours=i) for i in range(output_sequence)]\n",
        "\n",
        "trace_predicted = go.Scatter(x=date_values, y=predicted_forecast.reshape(-1), mode='lines', name='Predicted Data')\n",
        "\n",
        "layout = go.Layout(title='WDF for W30/2022', legend=dict(x=0, y=1, traceorder='normal', orientation='h'))\n",
        "fig = go.Figure(data=[trace_predicted], layout=layout)\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "040E6CElqpIg"
      },
      "outputs": [],
      "source": [
        "torch.Tensor(pca.transform(ss.transform(X_input[-input_sequence:].values)).reshape(1,(input_sequence),-1)).to(device).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBV7IaJZqpIg"
      },
      "outputs": [],
      "source": [
        "torch.Tensor(pca.transform(ss.transform(X_input[-input_sequence-output_sequence:-output_sequence].values)).reshape(1,input_sequence,-1)).to(device).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBezIJigqpIg"
      },
      "outputs": [],
      "source": [
        "X_input[-input_sequence:].values"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}